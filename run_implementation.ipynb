{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "from helpers_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_cleaned_csv_data(\"dataset\", sub_sample=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gamma: 0.001\n",
      "Current iteration=0, loss=0.4725981665543393\n",
      "Current iteration=100, loss=0.15143367794530063\n",
      "Current iteration=200, loss=0.14740982820692666\n",
      "Current iteration=300, loss=0.14504751612216937\n",
      "Current iteration=400, loss=0.14353656330365666\n",
      "Current iteration=500, loss=0.14249765162213124\n",
      "Current iteration=600, loss=0.1417322417086159\n",
      "Current iteration=700, loss=0.1411342887544156\n",
      "Current iteration=800, loss=0.14064549769620782\n",
      "Current iteration=900, loss=0.14023255044107097\n",
      "Gamma: 0.001, Best Threshold: -0.5920280549360802, F1 Score: 0.38206470667203657\n",
      "Testing gamma: 0.005\n",
      "Current iteration=0, loss=0.37436983598483686\n",
      "Current iteration=100, loss=0.14244954571221327\n",
      "Current iteration=200, loss=0.13985786968584088\n",
      "Current iteration=300, loss=0.1385871011683183\n",
      "Current iteration=400, loss=0.13778058231562196\n",
      "Current iteration=500, loss=0.13721341385677008\n",
      "Current iteration=600, loss=0.13678716133870586\n",
      "Current iteration=700, loss=0.13645124746421297\n",
      "Current iteration=800, loss=0.13617697455648614\n",
      "Current iteration=900, loss=0.1359468542520861\n",
      "Gamma: 0.005, Best Threshold: -0.5730878040606386, F1 Score: 0.40665064102564097\n",
      "Testing gamma: 0.01\n",
      "Current iteration=0, loss=0.2771871800025262\n",
      "Current iteration=100, loss=0.13983607420096908\n",
      "Current iteration=200, loss=0.13777164597567398\n",
      "Current iteration=300, loss=0.13678208660170768\n",
      "Current iteration=400, loss=0.1361736048470805\n",
      "Current iteration=500, loss=0.13574719533259838\n",
      "Current iteration=600, loss=0.13542442522262432\n",
      "Current iteration=700, loss=0.13516857487070075\n",
      "Current iteration=800, loss=0.13496008767181894\n",
      "Current iteration=900, loss=0.13478728462230952\n",
      "Gamma: 0.01, Best Threshold: -0.5899545505680638, F1 Score: 0.4070673168260402\n",
      "Testing gamma: 0.015\n",
      "Current iteration=0, loss=0.2084520320530683\n",
      "Current iteration=100, loss=0.13856103728487004\n",
      "Current iteration=200, loss=0.13677702789343468\n",
      "Current iteration=300, loss=0.13594115281591923\n",
      "Current iteration=400, loss=0.13542250798207145\n",
      "Current iteration=500, loss=0.135057845840651\n",
      "Current iteration=600, loss=0.13478618303436077\n",
      "Current iteration=700, loss=0.13457791999603969\n",
      "Current iteration=800, loss=0.13441562358778317\n",
      "Current iteration=900, loss=0.13428773508715597\n",
      "Gamma: 0.015, Best Threshold: -0.5745361112419721, F1 Score: 0.40823622881355937\n",
      "Testing gamma: 0.02\n",
      "Current iteration=0, loss=0.16816439213646314\n",
      "Current iteration=100, loss=0.1377538898859698\n",
      "Current iteration=200, loss=0.1361668898054392\n",
      "Current iteration=300, loss=0.1354205936517623\n",
      "Current iteration=400, loss=0.1349574907410779\n",
      "Current iteration=500, loss=0.13464058610697513\n",
      "Current iteration=600, loss=0.13441491758042082\n",
      "Current iteration=700, loss=0.1342507307935665\n",
      "Current iteration=800, loss=0.13412943662933408\n",
      "Current iteration=900, loss=0.13403865816038077\n",
      "Gamma: 0.02, Best Threshold: -0.5877618618259199, F1 Score: 0.407431133888533\n",
      "Best Gamma: 0.015\n",
      "Best Threshold: -0.5745361112419721\n",
      "Final Accuracy: 0.8638060554345011\n",
      "Final F1 Score: 0.40823622881355937\n"
     ]
    }
   ],
   "source": [
    "# Define a range of gamma values to test\n",
    "gamma_values = [0.001, 0.005, 0.01, 0.015, 0.02]\n",
    "\n",
    "# Initialize variables to store the best parameters\n",
    "best_gamma = None\n",
    "best_threshold = None\n",
    "best_f1 = 0\n",
    "best_w = None\n",
    "\n",
    "# Loop over gamma values\n",
    "for gamma in gamma_values:\n",
    "    print(f\"Testing gamma: {gamma}\")\n",
    "    # Initialize weights\n",
    "    initial_w = np.zeros(X_train.shape[1])\n",
    "    # Train the model\n",
    "    w, loss = mean_squared_error_gd(y_train, X_train, initial_w, max_iters=1000, gamma=gamma)\n",
    "\n",
    "    # Compute predicted scores on the validation set\n",
    "    y_scores = X_test @ w\n",
    "\n",
    "    # Optimize threshold for the current gamma\n",
    "    threshold, f1 = optimize_threshold(y_test, y_scores)\n",
    "\n",
    "    print(f\"Gamma: {gamma}, Best Threshold: {threshold}, F1 Score: {f1}\")\n",
    "\n",
    "    # Update best parameters if current F1 is better\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_gamma = gamma\n",
    "        best_threshold = threshold\n",
    "        best_w = w.copy()\n",
    "\n",
    "# After finding the best gamma and threshold, make predictions on the test set\n",
    "y_scores = X_test @ best_w\n",
    "y_prediction = np.where(y_scores >= best_threshold, 1, -1)\n",
    "\n",
    "# Compute final accuracy and F1 score\n",
    "final_accuracy = accuracy(y_test, y_prediction)\n",
    "final_f1_score = f1_score(y_test, y_prediction)\n",
    "\n",
    "print(\"Best Gamma:\", best_gamma)\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Final Accuracy:\", final_accuracy)\n",
    "print(\"Final F1 Score:\", final_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gamma: 0.0015\n",
      "Current iteration=0, loss=0.4464259998806827\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m initial_w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m w, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error_sgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Compute predicted scores on the validation set\u001b[39;00m\n\u001b[0;32m     20\u001b[0m y_scores \u001b[38;5;241m=\u001b[39m X_test \u001b[38;5;241m@\u001b[39m w\n",
      "File \u001b[1;32mc:\\Users\\poseidon\\Desktop\\AmelProject\\implementations.py:69\u001b[0m, in \u001b[0;36mmean_squared_error_sgd\u001b[1;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m     67\u001b[0m w \u001b[38;5;241m=\u001b[39m initial_w\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[1;32m---> 69\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mminibatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminibatch_tx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcompute_gradient_mse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminibatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminibatch_tx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\poseidon\\Desktop\\AmelProject\\helpers_implementations.py:70\u001b[0m, in \u001b[0;36mbatch_iter\u001b[1;34m(y, tx, batch_size, num_batches, shuffle)\u001b[0m\n\u001b[0;32m     67\u001b[0m data_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m---> 70\u001b[0m     shuffle_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(np\u001b[38;5;241m.\u001b[39marange(data_size))\n\u001b[0;32m     71\u001b[0m     shuffled_y \u001b[38;5;241m=\u001b[39m y[shuffle_indices]\n\u001b[0;32m     72\u001b[0m     shuffled_tx \u001b[38;5;241m=\u001b[39m tx[shuffle_indices]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define a range of gamma values to test\n",
    "\n",
    "gamma_values = [0.0015]\n",
    "\n",
    "# Initialize variables to store the best parameters\n",
    "best_gamma = None\n",
    "best_threshold = None\n",
    "best_f1 = 0\n",
    "best_w = None\n",
    "\n",
    "# Loop over gamma values\n",
    "for gamma in gamma_values:\n",
    "    print(f\"Testing gamma: {gamma}\")\n",
    "    # Initialize weights\n",
    "    initial_w = np.zeros(X_train.shape[1])\n",
    "    # Train the model\n",
    "    w, loss = mean_squared_error_sgd(y_train, X_train, initial_w, max_iters=3000, gamma=gamma)\n",
    "\n",
    "    # Compute predicted scores on the validation set\n",
    "    y_scores = X_test @ w\n",
    "\n",
    "    # Optimize threshold for the current gamma\n",
    "    threshold, f1 = optimize_threshold(y_test, y_scores)\n",
    "\n",
    "    print(f\"Gamma: {gamma}, Best Threshold: {threshold}, F1 Score: {f1}\")\n",
    "\n",
    "    # Update best parameters if current F1 is better\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_gamma = gamma\n",
    "        best_threshold = threshold\n",
    "        best_w = w.copy()\n",
    "\n",
    "# After finding the best gamma and threshold, make predictions on the test set\n",
    "y_scores = X_test @ best_w\n",
    "y_prediction = np.where(y_scores >= best_threshold, 1, -1)\n",
    "\n",
    "# Compute final accuracy and F1 score\n",
    "final_accuracy = accuracy(y_test, y_prediction)\n",
    "final_f1_score = f1_score(y_test, y_prediction)\n",
    "\n",
    "print(\"Best Gamma:\", best_gamma)\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Final Accuracy:\", final_accuracy)\n",
    "print(\"Final F1 Score:\", final_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: -0.5655146528668263\n",
      "Final Accuracy: 0.8670821460679293\n",
      "Final F1 Score: 0.4072035338090384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\poseidon\\Desktop\\AmelProject\\implementations.py:105: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w=np.linalg.lstsq(A,b)[0]\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(y_train, X_train)\n",
    "\n",
    "y_scores = X_test @ w\n",
    "\n",
    "# Optimize threshold for the current gamma\n",
    "threshold, f1 = optimize_threshold(y_test, y_scores)\n",
    "\n",
    "# After finding the best gamma and threshold, make predictions on the test set\n",
    "y_scores = X_test @ w\n",
    "y_prediction = np.where(y_scores >= threshold, 1, -1)\n",
    "\n",
    "# Compute final accuracy and F1 score\n",
    "final_accuracy = accuracy(y_test, y_prediction)\n",
    "final_f1_score = f1_score(y_test, y_prediction)\n",
    "\n",
    "print(\"Best Threshold:\", threshold)\n",
    "print(\"Final Accuracy:\", final_accuracy)\n",
    "print(\"Final F1 Score:\", final_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, _ = ridge_regression(y_train, X_train, lambda_=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gamma: 0.001\n",
      "Current iteration=0, loss=0.686190043604647\n",
      "Current iteration=100, loss=0.3813197240800137\n",
      "Current iteration=200, loss=0.3189964180323503\n",
      "Current iteration=300, loss=0.29846338414482615\n",
      "Current iteration=400, loss=0.2893554646429806\n"
     ]
    }
   ],
   "source": [
    "from helpers_analysis import *\n",
    "# Convert y_train and y_test to binary labels (0 and 1)\n",
    "y_train_binary = (y_train + 1) // 2\n",
    "y_test_binary = (y_test + 1) // 2\n",
    "\n",
    "# Define a range of gamma values to test\n",
    "gamma_values = [0.001,0.005,0.01,0.015,0.02]\n",
    "\n",
    "# Initialize variables to store the best parameters\n",
    "best_gamma = None\n",
    "best_threshold = None\n",
    "best_f1 = 0\n",
    "best_w = None\n",
    "\n",
    "# Loop over gamma values\n",
    "for gamma in gamma_values:\n",
    "    print(f\"Testing gamma: {gamma}\")\n",
    "    # Initialize weights\n",
    "    initial_w = np.zeros(X_train.shape[1])\n",
    "    # Train the model\n",
    "    w, loss = logistic_regression(y_train_binary, X_train, initial_w, max_iters=1000, gamma=gamma)\n",
    "    \n",
    "    # Compute predicted probabilities on the validation set\n",
    "    y_scores = sigmoid(X_test @ w)\n",
    "    \n",
    "    # Optimize threshold for the current gamma\n",
    "    threshold, f1 = optimize_threshold_logistic(y_test_binary, y_scores)\n",
    "    \n",
    "    print(f\"Gamma: {gamma}, Best Threshold: {threshold}, F1 Score: {f1}\")\n",
    "    \n",
    "    # Update best parameters if current F1 is better\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_gamma = gamma\n",
    "        best_threshold = threshold\n",
    "        best_w = w.copy()\n",
    "\n",
    "# After finding the best gamma and threshold, make predictions on the test set\n",
    "y_scores = sigmoid(X_test @ best_w)\n",
    "y_prediction_binary = np.where(y_scores >= best_threshold, 1, 0)\n",
    "y_prediction = y_prediction_binary * 2 - 1  # Convert back to -1 and 1\n",
    "\n",
    "# Compute final accuracy and F1 score\n",
    "final_accuracy = accuracy(y_test, y_prediction)\n",
    "final_f1_score = f1_score_logistic(y_test_binary, y_prediction_binary)\n",
    "\n",
    "print(\"Best Gamma:\", best_gamma)\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Final Accuracy:\", final_accuracy)\n",
    "print(\"Final F1 Score:\", final_f1_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

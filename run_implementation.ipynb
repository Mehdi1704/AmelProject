{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "import numpy as np\n",
    "from implementations import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(data_path, sub_sample=False):\n",
    "    \"\"\"\n",
    "    This function loads the data and returns the respectinve numpy arrays.\n",
    "    Remember to put the 3 files in the same folder and to not change the names of the files.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): datafolder path\n",
    "        sub_sample (bool, optional): If True the data will be subsempled. Default to False.\n",
    "\n",
    "    Returns:\n",
    "        x_train (np.array): training data\n",
    "        x_test (np.array): test data\n",
    "        y_train (np.array): labels for training data in format (-1,1)\n",
    "        train_ids (np.array): ids of training data\n",
    "        test_ids (np.array): ids of test data\n",
    "    \"\"\"\n",
    "    y_train = np.genfromtxt(\n",
    "        os.path.join(data_path, \"y_train.csv\"),\n",
    "        delimiter=\",\",\n",
    "        skip_header=1,\n",
    "        dtype=int,\n",
    "        usecols=1,\n",
    "    )\n",
    "    x_train = np.genfromtxt(\n",
    "        os.path.join(data_path, \"x_train_cleaned.csv\"), delimiter=\",\", skip_header=1\n",
    "    )\n",
    "    x_test = np.genfromtxt(\n",
    "        os.path.join(data_path, \"x_test_cleaned.csv\"), delimiter=\",\", skip_header=1\n",
    "    )\n",
    "\n",
    "    train_ids = x_train[:, 0].astype(dtype=int)\n",
    "    test_ids = x_test[:, 0].astype(dtype=int)\n",
    "    x_train = x_train[:, 1:]\n",
    "    x_test = x_test[:, 1:]\n",
    "\n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        y_train = y_train[::50]\n",
    "        x_train = x_train[::50]\n",
    "        train_ids = train_ids[::50]\n",
    "\n",
    "    return x_train, x_test, y_train, train_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"dataset\", sub_sample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w=np.zeros((x_train.shape[1],))\n",
    "gamma=0.1\n",
    "max_iters=100\n",
    "y_train = y_train[train_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy between true labels and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (numpy array): True labels\n",
    "    y_pred (numpy array): Predicted labels\n",
    "\n",
    "    Returns:\n",
    "    float: Accuracy score\n",
    "    \"\"\"\n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    total = len(y_true)\n",
    "    return correct / total\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the F1-score between true labels and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (numpy array): True labels\n",
    "    y_pred (numpy array): Predicted labels\n",
    "\n",
    "    Returns:\n",
    "    float: F1-score\n",
    "    \"\"\"\n",
    "    # True Positives (TP)\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    # False Positives (FP)\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    # False Negatives (FN)\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    # Avoid division by zero\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "    if (precision + recall) == 0:\n",
    "        return 0\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_mse2(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    N = len(y)\n",
    "    error = y - tx.dot(w)\n",
    "    gradient = -1 / N * tx.T.dot(error)\n",
    "    return gradient\n",
    "\n",
    "def compute_loss_mse2(y, tx, w):\n",
    "    \"\"\"Calculate the loss using mse.\"\"\"\n",
    "    N = len(y)\n",
    "    squared_error = (y - tx.dot(w)) ** 2\n",
    "    loss = 1 / (2 * N) * np.sum(squared_error)\n",
    "    return loss\n",
    "\n",
    "def mean_squared_error_gd2(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Calculate the loss using mse\"\"\"\n",
    "    if max_iters == 0:\n",
    "        return (initial_w, compute_loss_mse2(y, tx, initial_w))\n",
    "    w = initial_w\n",
    "    # loss = compute_loss_mse(y, tx, w)\n",
    "    for iter in range(max_iters):\n",
    "        gradient = compute_gradient_mse2(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "        if iter % 100 == 0:\n",
    "            print(\n",
    "                \"Current iteration={i}, loss={l}\".format(\n",
    "                    i=iter, l=compute_loss_mse2(y, tx, w)\n",
    "                )\n",
    "            )\n",
    "    loss = compute_loss_mse2(y, tx, w)\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: -1.0452428976193437\n",
      "Accuracy with optimized threshold: 0.13686440032303776\n",
      "F1 Score with optimized threshold: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def optimize_threshold(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Finds the threshold that maximizes the F1 score.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (numpy array): True labels (-1 or 1)\n",
    "    y_scores (numpy array): Predicted scores from the model\n",
    "\n",
    "    Returns:\n",
    "    best_threshold (float): Threshold that gives the highest F1 score\n",
    "    best_f1 (float): The highest F1 score achieved\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    best_threshold = None\n",
    "    best_f1 = -1\n",
    "\n",
    "    # Define thresholds based on percentiles to avoid extreme values\n",
    "    score_min = np.percentile(y_scores, 5)\n",
    "    score_max = np.percentile(y_scores, 95)\n",
    "    thresholds = np.linspace(score_min, score_max, 100)\n",
    "\n",
    "    # Iterate over thresholds to find the best one\n",
    "    for threshold in thresholds:\n",
    "        # Convert scores to binary predictions using the threshold\n",
    "        y_pred = np.where(y_scores >= threshold, 1, -1)\n",
    "\n",
    "        # Compute the F1 score for these predictions\n",
    "        current_f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "        # Update best threshold if current F1 is better\n",
    "        if current_f1 > best_f1:\n",
    "            best_f1 = current_f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.4725981665543393\n",
      "Current iteration=100, loss=0.15143367794530063\n",
      "Current iteration=200, loss=0.14740982820692666\n",
      "Current iteration=300, loss=0.14504751612216937\n",
      "Current iteration=400, loss=0.14353656330365666\n",
      "Best Threshold: -0.6331050033031871\n",
      "Accuracy with optimized threshold: 0.8472884635896811\n",
      "F1 Score with optimized threshold: 0.36279247202441506\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Initialize weights\n",
    "initial_w = np.zeros(X_train.shape[1])\n",
    "\n",
    "# Train your model\n",
    "w, loss = mean_squared_error_gd2(y_train, X_train, initial_w, 500, 0.001)\n",
    "\n",
    "# Compute predicted scores on the test set\n",
    "y_scores = X_test @ w\n",
    "\n",
    "\n",
    "# Optimize threshold\n",
    "best_threshold, best_f1 = optimize_threshold(y_test, y_scores)\n",
    "\n",
    "# Make final predictions\n",
    "y_prediction = np.where(y_scores >= best_threshold, 1, -1)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_score = accuracy(y_test, y_prediction)\n",
    "final_f1_score = f1_score(y_test, y_prediction)\n",
    "\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Accuracy with optimized threshold:\", accuracy_score)\n",
    "print(\"F1 Score with optimized threshold:\", final_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gamma: 0.001\n",
      "Current iteration=0, loss=0.4725981665543393\n",
      "Current iteration=100, loss=0.15143367794530063\n",
      "Current iteration=200, loss=0.14740982820692666\n",
      "Current iteration=300, loss=0.14504751612216937\n",
      "Current iteration=400, loss=0.14353656330365666\n",
      "Current iteration=500, loss=0.14249765162213124\n",
      "Current iteration=600, loss=0.1417322417086159\n",
      "Current iteration=700, loss=0.1411342887544156\n",
      "Current iteration=800, loss=0.14064549769620782\n",
      "Current iteration=900, loss=0.14023255044107097\n",
      "Gamma: 0.001, Best Threshold: -0.5920280549360802, F1 Score: 0.38206470667203657\n",
      "Testing gamma: 0.005\n",
      "Current iteration=0, loss=0.37436983598483686\n",
      "Current iteration=100, loss=0.14244954571221327\n",
      "Current iteration=200, loss=0.13985786968584088\n",
      "Current iteration=300, loss=0.1385871011683183\n",
      "Current iteration=400, loss=0.13778058231562196\n",
      "Current iteration=500, loss=0.13721341385677008\n",
      "Current iteration=600, loss=0.13678716133870586\n",
      "Current iteration=700, loss=0.13645124746421297\n",
      "Current iteration=800, loss=0.13617697455648614\n",
      "Current iteration=900, loss=0.1359468542520861\n",
      "Gamma: 0.005, Best Threshold: -0.5730878040606386, F1 Score: 0.40665064102564097\n",
      "Testing gamma: 0.01\n",
      "Current iteration=0, loss=0.2771871800025262\n",
      "Current iteration=100, loss=0.13983607420096908\n",
      "Current iteration=200, loss=0.13777164597567398\n",
      "Current iteration=300, loss=0.13678208660170768\n",
      "Current iteration=400, loss=0.1361736048470805\n",
      "Current iteration=500, loss=0.13574719533259838\n",
      "Current iteration=600, loss=0.13542442522262432\n",
      "Current iteration=700, loss=0.13516857487070075\n",
      "Current iteration=800, loss=0.13496008767181894\n",
      "Current iteration=900, loss=0.13478728462230952\n",
      "Gamma: 0.01, Best Threshold: -0.5899545505680638, F1 Score: 0.4070673168260402\n",
      "Testing gamma: 0.015\n",
      "Current iteration=0, loss=0.2084520320530683\n",
      "Current iteration=100, loss=0.13856103728487004\n",
      "Current iteration=200, loss=0.13677702789343468\n",
      "Current iteration=300, loss=0.13594115281591923\n",
      "Current iteration=400, loss=0.13542250798207145\n",
      "Current iteration=500, loss=0.135057845840651\n",
      "Current iteration=600, loss=0.13478618303436077\n",
      "Current iteration=700, loss=0.13457791999603969\n",
      "Current iteration=800, loss=0.13441562358778317\n",
      "Current iteration=900, loss=0.13428773508715597\n",
      "Gamma: 0.015, Best Threshold: -0.5745361112419721, F1 Score: 0.40823622881355937\n",
      "Testing gamma: 0.02\n",
      "Current iteration=0, loss=0.16816439213646314\n",
      "Current iteration=100, loss=0.1377538898859698\n",
      "Current iteration=200, loss=0.1361668898054392\n",
      "Current iteration=300, loss=0.1354205936517623\n",
      "Current iteration=400, loss=0.1349574907410779\n",
      "Current iteration=500, loss=0.13464058610697513\n",
      "Current iteration=600, loss=0.13441491758042082\n",
      "Current iteration=700, loss=0.1342507307935665\n",
      "Current iteration=800, loss=0.13412943662933408\n",
      "Current iteration=900, loss=0.13403865816038077\n",
      "Gamma: 0.02, Best Threshold: -0.5877618618259199, F1 Score: 0.407431133888533\n",
      "Best Gamma: 0.015\n",
      "Best Threshold: -0.5745361112419721\n",
      "Final Accuracy: 0.8638060554345011\n",
      "Final F1 Score: 0.40823622881355937\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a range of gamma values to test\n",
    "gamma_values = [0.001, 0.005, 0.01, 0.015, 0.02]\n",
    "\n",
    "# Initialize variables to store the best parameters\n",
    "best_gamma = None\n",
    "best_threshold = None\n",
    "best_f1 = -1\n",
    "best_w = None\n",
    "\n",
    "# Function to optimize threshold\n",
    "def optimize_threshold(y_true, y_scores):\n",
    "    best_threshold = None\n",
    "    best_f1 = -1\n",
    "\n",
    "    # Generate a list of potential thresholds to try\n",
    "    thresholds = np.linspace(np.min(y_scores), np.max(y_scores), 100)\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = np.where(y_scores >= threshold, 1, -1)\n",
    "        current_f1 = f1_score(y_true, y_pred)\n",
    "        if current_f1 > best_f1:\n",
    "            best_f1 = current_f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "# Loop over gamma values\n",
    "for gamma in gamma_values:\n",
    "    print(f\"Testing gamma: {gamma}\")\n",
    "    # Initialize weights\n",
    "    initial_w = np.zeros(X_train.shape[1])\n",
    "    # Train the model\n",
    "    w, loss = mean_squared_error_gd2(y_train, X_train, initial_w, max_iters=1000, gamma=gamma)\n",
    "\n",
    "    # Compute predicted scores on the validation set\n",
    "    y_scores = X_test @ w\n",
    "\n",
    "    # Optimize threshold for the current gamma\n",
    "    threshold, f1 = optimize_threshold(y_test, y_scores)\n",
    "\n",
    "    print(f\"Gamma: {gamma}, Best Threshold: {threshold}, F1 Score: {f1}\")\n",
    "\n",
    "    # Update best parameters if current F1 is better\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_gamma = gamma\n",
    "        best_threshold = threshold\n",
    "        best_w = w.copy()\n",
    "\n",
    "# After finding the best gamma and threshold, make predictions on the test set\n",
    "y_scores = X_test @ best_w\n",
    "y_prediction = np.where(y_scores >= best_threshold, 1, -1)\n",
    "\n",
    "# Compute final accuracy and F1 score\n",
    "final_accuracy = accuracy(y_test, y_prediction)\n",
    "final_f1_score = f1_score(y_test, y_prediction)\n",
    "\n",
    "print(\"Best Gamma:\", best_gamma)\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Final Accuracy:\", final_accuracy)\n",
    "print(\"Final F1 Score:\", final_f1_score)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

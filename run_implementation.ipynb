{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "from helpers_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_cleaned_csv_data(\"dataset\", sub_sample=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gamma: 0.001\n",
      "Current iteration=0, loss=0.4725981665543394\n",
      "Current iteration=100, loss=0.15143367794530063\n",
      "Current iteration=200, loss=0.14740982820692666\n",
      "Current iteration=300, loss=0.14504751612216937\n",
      "Current iteration=400, loss=0.14353656330365666\n",
      "Current iteration=500, loss=0.14249765162213124\n",
      "Current iteration=600, loss=0.1417322417086159\n",
      "Current iteration=700, loss=0.1411342887544156\n",
      "Current iteration=800, loss=0.14064549769620782\n",
      "Current iteration=900, loss=0.14023255044107094\n",
      "Gamma: 0.001, Best Threshold: -0.5920280549360802, F1 Score: 0.38206470667203657\n",
      "Testing gamma: 0.005\n",
      "Current iteration=0, loss=0.3743698359848376\n",
      "Current iteration=100, loss=0.14244954571221327\n",
      "Current iteration=200, loss=0.13985786968584088\n",
      "Current iteration=300, loss=0.1385871011683183\n",
      "Current iteration=400, loss=0.13778058231562196\n",
      "Current iteration=500, loss=0.13721341385677008\n",
      "Current iteration=600, loss=0.13678716133870586\n",
      "Current iteration=700, loss=0.13645124746421294\n",
      "Current iteration=800, loss=0.13617697455648614\n",
      "Current iteration=900, loss=0.1359468542520861\n",
      "Gamma: 0.005, Best Threshold: -0.5730878040606388, F1 Score: 0.40665064102564097\n",
      "Testing gamma: 0.01\n",
      "Current iteration=0, loss=0.27718718000252734\n",
      "Current iteration=100, loss=0.13983607420096908\n",
      "Current iteration=200, loss=0.13777164597567398\n",
      "Current iteration=300, loss=0.13678208660170768\n",
      "Current iteration=400, loss=0.1361736048470805\n",
      "Current iteration=500, loss=0.13574719533259838\n",
      "Current iteration=600, loss=0.13542442522262432\n",
      "Current iteration=700, loss=0.13516857487070075\n",
      "Current iteration=800, loss=0.13496008767181894\n",
      "Current iteration=900, loss=0.13478728462230952\n",
      "Gamma: 0.01, Best Threshold: -0.5899545505680636, F1 Score: 0.4070673168260402\n",
      "Testing gamma: 0.015\n",
      "Current iteration=0, loss=0.20845203205306947\n",
      "Current iteration=100, loss=0.13856103728487004\n",
      "Current iteration=200, loss=0.13677702789343468\n",
      "Current iteration=300, loss=0.13594115281591923\n",
      "Current iteration=400, loss=0.1354225079820714\n",
      "Current iteration=500, loss=0.135057845840651\n",
      "Current iteration=600, loss=0.13478618303436077\n",
      "Current iteration=700, loss=0.13457791999603969\n",
      "Current iteration=800, loss=0.13441562358778317\n",
      "Current iteration=900, loss=0.13428773508715597\n",
      "Gamma: 0.015, Best Threshold: -0.5745361112419722, F1 Score: 0.40823622881355937\n",
      "Testing gamma: 0.02\n",
      "Current iteration=0, loss=0.16816439213646386\n",
      "Current iteration=100, loss=0.1377538898859698\n",
      "Current iteration=200, loss=0.1361668898054392\n",
      "Current iteration=300, loss=0.13542059365176232\n",
      "Current iteration=400, loss=0.1349574907410779\n",
      "Current iteration=500, loss=0.13464058610697513\n",
      "Current iteration=600, loss=0.13441491758042082\n",
      "Current iteration=700, loss=0.1342507307935665\n",
      "Current iteration=800, loss=0.13412943662933408\n",
      "Current iteration=900, loss=0.13403865816038074\n",
      "Gamma: 0.02, Best Threshold: -0.5877618618259199, F1 Score: 0.407431133888533\n",
      "Best Gamma: 0.015\n",
      "Best Threshold: -0.5745361112419722\n",
      "Final Accuracy: 0.8638060554345011\n",
      "Final F1 Score: 0.40823622881355937\n"
     ]
    }
   ],
   "source": [
    "# Define a range of gamma values to test\n",
    "gamma_values = [0.001, 0.005, 0.01, 0.015, 0.02]\n",
    "\n",
    "# Initialize variables to store the best parameters\n",
    "best_gamma = None\n",
    "best_threshold = None\n",
    "best_f1 = 0\n",
    "best_w = None\n",
    "\n",
    "# Loop over gamma values\n",
    "for gamma in gamma_values:\n",
    "    print(f\"Testing gamma: {gamma}\")\n",
    "    # Initialize weights\n",
    "    initial_w = np.zeros(X_train.shape[1])\n",
    "    # Train the model\n",
    "    w, loss = mean_squared_error_gd(y_train, X_train, initial_w, max_iters=1000, gamma=gamma)\n",
    "\n",
    "    # Compute predicted scores on the validation set\n",
    "    y_scores = X_test @ w\n",
    "\n",
    "    # Optimize threshold for the current gamma\n",
    "    threshold, f1 = optimize_threshold(y_test, y_scores)\n",
    "\n",
    "    print(f\"Gamma: {gamma}, Best Threshold: {threshold}, F1 Score: {f1}\")\n",
    "\n",
    "    # Update best parameters if current F1 is better\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_gamma = gamma\n",
    "        best_threshold = threshold\n",
    "        best_w = w.copy()\n",
    "\n",
    "# After finding the best gamma and threshold, make predictions on the test set\n",
    "y_scores = X_test @ best_w\n",
    "y_prediction = np.where(y_scores >= best_threshold, 1, -1)\n",
    "\n",
    "# Compute final accuracy and F1 score\n",
    "final_accuracy = accuracy(y_test, y_prediction)\n",
    "final_f1_score = f1_score(y_test, y_prediction)\n",
    "\n",
    "print(\"Best Gamma:\", best_gamma)\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Final Accuracy:\", final_accuracy)\n",
    "print(\"Final F1 Score:\", final_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gamma: 0.0015\n",
      "Current iteration=0, loss=0.44463934577221786\n",
      "Current iteration=100, loss=0.1536417498581622\n",
      "Current iteration=200, loss=0.1527000247788605\n",
      "Current iteration=300, loss=0.1467742804091567\n",
      "Current iteration=400, loss=0.16052972080716785\n",
      "Current iteration=500, loss=0.14382174344569126\n",
      "Current iteration=600, loss=0.14149067332226395\n",
      "Current iteration=700, loss=0.1443152339632158\n",
      "Current iteration=800, loss=0.1410103203473426\n",
      "Current iteration=900, loss=0.14082586486096027\n",
      "Current iteration=1000, loss=0.14003749838646493\n",
      "Current iteration=1100, loss=0.1426118654502608\n",
      "Current iteration=1200, loss=0.21702321732243424\n",
      "Current iteration=1300, loss=0.14106596139084612\n",
      "Current iteration=1400, loss=0.1420982360285399\n",
      "Current iteration=1500, loss=0.14255688163109942\n",
      "Current iteration=1600, loss=0.14528033315385386\n",
      "Current iteration=1700, loss=0.14228360961855335\n",
      "Current iteration=1800, loss=0.1409587822669644\n",
      "Current iteration=1900, loss=0.14223409749811566\n",
      "Current iteration=2000, loss=0.1402866667343303\n",
      "Current iteration=2100, loss=0.13976914840234023\n",
      "Current iteration=2200, loss=0.14672183111745557\n",
      "Current iteration=2300, loss=0.14410609903136817\n",
      "Current iteration=2400, loss=0.14155305372976326\n",
      "Current iteration=2500, loss=0.13955551894878981\n",
      "Current iteration=2600, loss=0.1470914568541284\n",
      "Current iteration=2700, loss=0.13958187075898415\n",
      "Current iteration=2800, loss=0.14226903009166195\n",
      "Current iteration=2900, loss=0.14333854124266493\n",
      "Gamma: 0.0015, Best Threshold: -0.3614449773703785, F1 Score: 0.3957552175451008\n",
      "Best Gamma: 0.0015\n",
      "Best Threshold: -0.3614449773703785\n",
      "Final Accuracy: 0.8698553948832035\n",
      "Final F1 Score: 0.3957552175451008\n"
     ]
    }
   ],
   "source": [
    "# Define a range of gamma values to test\n",
    "\n",
    "gamma_values = [0.0015]\n",
    "\n",
    "# Initialize variables to store the best parameters\n",
    "best_gamma = None\n",
    "best_threshold = None\n",
    "best_f1 = 0\n",
    "best_w = None\n",
    "\n",
    "# Loop over gamma values\n",
    "for gamma in gamma_values:\n",
    "    print(f\"Testing gamma: {gamma}\")\n",
    "    # Initialize weights\n",
    "    initial_w = np.zeros(X_train.shape[1])\n",
    "    # Train the model\n",
    "    w, loss = mean_squared_error_sgd(y_train, X_train, initial_w, max_iters=3000, gamma=gamma)\n",
    "\n",
    "    # Compute predicted scores on the validation set\n",
    "    y_scores = X_test @ w\n",
    "\n",
    "    # Optimize threshold for the current gamma\n",
    "    threshold, f1 = optimize_threshold(y_test, y_scores)\n",
    "\n",
    "    print(f\"Gamma: {gamma}, Best Threshold: {threshold}, F1 Score: {f1}\")\n",
    "\n",
    "    # Update best parameters if current F1 is better\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_gamma = gamma\n",
    "        best_threshold = threshold\n",
    "        best_w = w.copy()\n",
    "\n",
    "# After finding the best gamma and threshold, make predictions on the test set\n",
    "y_scores = X_test @ best_w\n",
    "y_prediction = np.where(y_scores >= best_threshold, 1, -1)\n",
    "\n",
    "# Compute final accuracy and F1 score\n",
    "final_accuracy = accuracy(y_test, y_prediction)\n",
    "final_f1_score = f1_score(y_test, y_prediction)\n",
    "\n",
    "print(\"Best Gamma:\", best_gamma)\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Final Accuracy:\", final_accuracy)\n",
    "print(\"Final F1 Score:\", final_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Best Threshold: -0.5655146573621888\n",
      "Final Accuracy: 0.8670821460679293\n",
      "Final F1 Score: 0.4072035338090384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aliridhamrad/Desktop/AmelProject/implementations.py:105: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w=np.linalg.lstsq(A,b)[0]\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(y_train, X_train)\n",
    "\n",
    "y_scores = X_test @ w\n",
    "\n",
    "# Optimize threshold for the current gamma\n",
    "threshold, f1 = optimize_threshold(y_test, y_scores)\n",
    "\n",
    "# After finding the best gamma and threshold, make predictions on the test set\n",
    "y_scores = X_test @ w\n",
    "y_prediction = np.where(y_scores >= threshold, 1, -1)\n",
    "\n",
    "# Compute final accuracy and F1 score\n",
    "final_accuracy = accuracy(y_test, y_prediction)\n",
    "final_f1_score = f1_score(y_test, y_prediction)\n",
    "\n",
    "print(\"Best Threshold:\", threshold)\n",
    "print(\"Final Accuracy:\", final_accuracy)\n",
    "print(\"Final F1 Score:\", final_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "w, _ = ridge_regression(y_train, X_train, lambda_=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gamma: 0.001\n",
      "Current iteration=0, loss=0.686190043604647\n",
      "Current iteration=100, loss=0.3813197240800137\n",
      "Current iteration=200, loss=0.3189964180323503\n",
      "Current iteration=300, loss=0.29846338414482615\n",
      "Current iteration=400, loss=0.28935546464298073\n",
      "Current iteration=500, loss=0.28431286434650865\n",
      "Current iteration=600, loss=0.2809723300223471\n",
      "Current iteration=700, loss=0.27843569422547215\n",
      "Current iteration=800, loss=0.2763222610993306\n",
      "Current iteration=900, loss=0.27445783879813723\n",
      "Gamma: 0.001, Best Threshold: 0.044235301839268576, F1 Score: 1.0\n",
      "Testing gamma: 0.005\n",
      "Current iteration=0, loss=0.6590724115869737\n",
      "Current iteration=100, loss=0.2840282244249369\n",
      "Current iteration=200, loss=0.27266893710878976\n",
      "Current iteration=300, loss=0.2656514142742886\n",
      "Current iteration=400, loss=0.26025352165742227\n",
      "Current iteration=500, loss=0.2560158880993777\n",
      "Current iteration=600, loss=0.25264180541897896\n",
      "Current iteration=700, loss=0.24991477761311398\n",
      "Current iteration=800, loss=0.2476770794146025\n",
      "Current iteration=900, loss=0.24581368499095463\n",
      "Gamma: 0.005, Best Threshold: 0.014721067505086546, F1 Score: 1.0\n",
      "Testing gamma: 0.01\n",
      "Current iteration=0, loss=0.6267717812660579\n",
      "Current iteration=100, loss=0.2725586086709882\n",
      "Current iteration=200, loss=0.2601867214715242\n",
      "Current iteration=300, loss=0.2525972510851191\n",
      "Current iteration=400, loss=0.2476461953681938\n",
      "Current iteration=500, loss=0.24421792338563852\n",
      "Current iteration=600, loss=0.24171265413669032\n",
      "Current iteration=700, loss=0.23979710819613906\n",
      "Current iteration=800, loss=0.23827802001188444\n",
      "Current iteration=900, loss=0.23703813356465092\n",
      "Gamma: 0.01, Best Threshold: 0.007793666726264702, F1 Score: 1.0\n",
      "Testing gamma: 0.015\n",
      "Current iteration=0, loss=0.5962354921438843\n",
      "Current iteration=100, loss=0.26548573073482135\n",
      "Current iteration=200, loss=0.2525527664576376\n",
      "Current iteration=300, loss=0.24576149853975424\n",
      "Current iteration=400, loss=0.24169592586608396\n",
      "Current iteration=500, loss=0.23898501900467894\n",
      "Current iteration=600, loss=0.23702956508858522\n",
      "Current iteration=700, loss=0.23553957827799332\n",
      "Current iteration=800, loss=0.2343600055584069\n",
      "Current iteration=900, loss=0.23340010145331827\n",
      "Gamma: 0.015, Best Threshold: 0.005802559760924957, F1 Score: 1.0\n",
      "Testing gamma: 0.02\n",
      "Current iteration=0, loss=0.5674473794964175\n",
      "Current iteration=100, loss=0.2600533690594748\n",
      "Current iteration=200, loss=0.2475845839577034\n",
      "Current iteration=300, loss=0.2416792269326045\n",
      "Current iteration=400, loss=0.23825718325509443\n",
      "Current iteration=500, loss=0.2359886759833641\n",
      "Current iteration=600, loss=0.23435471224115692\n",
      "Current iteration=700, loss=0.23311431815197428\n",
      "Current iteration=800, loss=0.2321381657862619\n",
      "Current iteration=900, loss=0.23134889014183901\n",
      "Gamma: 0.02, Best Threshold: 0.004897644097851516, F1 Score: 1.0\n",
      "Best Gamma: 0.001\n",
      "Best Threshold: 0.044235301839268576\n",
      "Final Accuracy: 0.0870678226949274\n",
      "Final F1 Score: 0.16018839096732596\n"
     ]
    }
   ],
   "source": [
    "from helpers_analysis import *\n",
    "# Convert y_train and y_test to binary labels (0 and 1)\n",
    "y_train_binary = (y_train + 1) // 2\n",
    "y_test_binary = (y_test + 1) // 2\n",
    "\n",
    "# Define a range of gamma values to test\n",
    "gamma_values = [0.001,0.005,0.01,0.015,0.02]\n",
    "\n",
    "# Initialize variables to store the best parameters\n",
    "best_gamma = None\n",
    "best_threshold = None\n",
    "best_f1 = 0\n",
    "best_w = None\n",
    "\n",
    "# Loop over gamma values\n",
    "for gamma in gamma_values:\n",
    "    print(f\"Testing gamma: {gamma}\")\n",
    "    # Initialize weights\n",
    "    initial_w = np.zeros(X_train.shape[1])\n",
    "    # Train the model\n",
    "    w, loss = logistic_regression(y_train_binary, X_train, initial_w, max_iters=1000, gamma=gamma)\n",
    "    \n",
    "    # Compute predicted probabilities on the validation set\n",
    "    y_scores = sigmoid(X_test @ w)\n",
    "    \n",
    "    # Optimize threshold for the current gamma\n",
    "    threshold, f1 = optimize_threshold_logistic(y_test_binary, y_scores)\n",
    "    \n",
    "    print(f\"Gamma: {gamma}, Best Threshold: {threshold}, F1 Score: {f1}\")\n",
    "    \n",
    "    # Update best parameters if current F1 is better\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_gamma = gamma\n",
    "        best_threshold = threshold\n",
    "        best_w = w.copy()\n",
    "\n",
    "# After finding the best gamma and threshold, make predictions on the test set\n",
    "y_scores = sigmoid(X_test @ best_w)\n",
    "y_prediction_binary = np.where(y_scores >= best_threshold, 1, 0)\n",
    "y_prediction = y_prediction_binary * 2 - 1  # Convert back to -1 and 1\n",
    "\n",
    "# Compute final accuracy and F1 score\n",
    "final_accuracy = accuracy(y_test, y_prediction)\n",
    "final_f1_score = f1_score_logistic(y_test_binary, y_prediction_binary)\n",
    "\n",
    "print(\"Best Gamma:\", best_gamma)\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Final Accuracy:\", final_accuracy)\n",
    "print(\"Final F1 Score:\", final_f1_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

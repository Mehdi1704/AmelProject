{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "from helpers_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_cleaned_csv_data(\"dataset\", sub_sample=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gamma: 0.005\n",
      "Current iteration=0, loss=0.3490784161307585\n",
      "Current iteration=100, loss=0.1423381210160878\n",
      "Current iteration=200, loss=0.13956501944015492\n",
      "Current iteration=300, loss=0.13825976138033838\n",
      "Current iteration=400, loss=0.13746060345020664\n",
      "Current iteration=500, loss=0.13691135783591357\n",
      "Current iteration=600, loss=0.1365027881955025\n",
      "Current iteration=700, loss=0.13618111588770956\n",
      "Current iteration=800, loss=0.1359173293030289\n",
      "Current iteration=900, loss=0.13569461052491605\n",
      "Gamma: 0.005, Best Threshold: -0.5874688035903483, F1 Score: 0.409984, Accuracy: 0.8595242811647645\n",
      "Testing gamma: 0.01\n",
      "Current iteration=0, loss=0.2418648937437182\n",
      "Current iteration=100, loss=0.13954185361936908\n",
      "Current iteration=200, loss=0.13745173280410217\n",
      "Current iteration=300, loss=0.13649793028963877\n",
      "Current iteration=400, loss=0.13591410946029936\n",
      "Current iteration=500, loss=0.13550021554716726\n",
      "Current iteration=600, loss=0.1351840110930896\n",
      "Current iteration=700, loss=0.134932765538862\n",
      "Current iteration=800, loss=0.13472864822320876\n",
      "Current iteration=900, loss=0.1345604879400959\n",
      "Gamma: 0.01, Best Threshold: -0.5633077453650329, F1 Score: 0.4115029099623417, Accuracy: 0.8690325628171331\n",
      "Testing gamma: 0.015\n",
      "Current iteration=0, loss=0.17835943283887876\n",
      "Current iteration=100, loss=0.13823303924241137\n",
      "Current iteration=200, loss=0.13649308899847767\n",
      "Current iteration=300, loss=0.13568913070510566\n",
      "Current iteration=400, loss=0.13518213275733443\n",
      "Current iteration=500, loss=0.13482422103360947\n",
      "Current iteration=600, loss=0.13455940714633768\n",
      "Current iteration=700, loss=0.13435857723434552\n",
      "Current iteration=800, loss=0.1342037855559054\n",
      "Current iteration=900, loss=0.13408299628412446\n",
      "Gamma: 0.015, Best Threshold: -0.5647802985057687, F1 Score: 0.41063656147986943, Accuracy: 0.8679506910265592\n",
      "Testing gamma: 0.02\n",
      "Current iteration=0, loss=0.15856203341624014\n",
      "Current iteration=100, loss=0.13743412097253552\n",
      "Current iteration=200, loss=0.13590769311193104\n",
      "Current iteration=300, loss=0.13518025705057854\n",
      "Current iteration=400, loss=0.1347260897743928\n",
      "Current iteration=500, loss=0.1344187526164257\n",
      "Current iteration=600, loss=0.13420310693325313\n",
      "Current iteration=700, loss=0.13404824351233105\n",
      "Current iteration=800, loss=0.13393499229632325\n",
      "Current iteration=900, loss=0.13385084575221295\n",
      "Gamma: 0.02, Best Threshold: -0.5778628201224768, F1 Score: 0.4092339979013641, Accuracy: 0.8627394212747802\n",
      "Best Gamma: 0.01\n",
      "Best Threshold: -0.5633077453650329\n",
      "Final Accuracy: 0.8690325628171331\n",
      "Final F1 Score: 0.4115029099623417\n"
     ]
    }
   ],
   "source": [
    "# Define a range of gamma values to test\n",
    "gamma_values = [0.005, 0.01, 0.015, 0.02]\n",
    "\n",
    "# Initialize variables to store the best parameters\n",
    "best_gamma = None\n",
    "best_threshold = None\n",
    "best_f1 = 0\n",
    "best_w = None\n",
    "\n",
    "# Loop over gamma values\n",
    "for gamma in gamma_values:\n",
    "    print(f\"Testing gamma: {gamma}\")\n",
    "    # Initialize weights\n",
    "    initial_w = np.zeros(X_train.shape[1])\n",
    "    # Train the model\n",
    "    w, loss = mean_squared_error_gd(y_train, X_train, initial_w, max_iters=1000, gamma=gamma)\n",
    "\n",
    "    # Compute predicted scores on the validation set\n",
    "    y_scores = X_test @ w\n",
    "\n",
    "    # Optimize threshold for the current gamma\n",
    "    threshold, f1, acc = optimize_threshold(y_test, y_scores)\n",
    "\n",
    "    print(f\"Gamma: {gamma}, Best Threshold: {threshold}, F1 Score: {f1}, Accuracy: {acc}\")\n",
    "\n",
    "    # Update best parameters if current F1 is better\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_gamma = gamma\n",
    "        best_threshold = threshold\n",
    "        best_w = w.copy()\n",
    "\n",
    "# After finding the best gamma and threshold, make predictions on the test set\n",
    "y_scores = X_test @ best_w\n",
    "y_prediction = np.where(y_scores >= best_threshold, 1, -1)\n",
    "\n",
    "# Compute final accuracy and F1 score\n",
    "final_accuracy = accuracy(y_test, y_prediction)\n",
    "final_f1_score = f1_score(y_test, y_prediction)\n",
    "\n",
    "print(\"Best Gamma:\", best_gamma)\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Final Accuracy:\", final_accuracy)\n",
    "print(\"Final F1 Score:\", final_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gamma: 0.0015\n",
      "Current iteration=0, loss=0.4358293071877409\n",
      "Current iteration=100, loss=0.1554699364256183\n",
      "Current iteration=200, loss=0.15391516933068794\n",
      "Current iteration=300, loss=0.14794901944581687\n",
      "Current iteration=400, loss=0.14580763522809306\n",
      "Current iteration=500, loss=0.14309168487530463\n",
      "Current iteration=600, loss=0.14533765651766442\n",
      "Current iteration=700, loss=0.14200531966951538\n",
      "Current iteration=800, loss=0.1418814539905092\n",
      "Current iteration=900, loss=0.14739712831977322\n",
      "Gamma: 0.0015, Best Threshold: -0.6023182366929547, F1 Score: 0.3757095709570957, Accuracy: 0.8558824873908605\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m y_prediction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y_scores \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m best_threshold, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Compute final accuracy and F1 score\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m final_accuracy \u001b[38;5;241m=\u001b[39m accuracy(y_test, y_prediction)\n\u001b[1;32m     40\u001b[0m final_f1_score \u001b[38;5;241m=\u001b[39m f1_score(y_test, y_prediction)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Gamma:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_gamma)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "# Define a range of gamma values to test\n",
    "\n",
    "gamma_values = [0.0015]\n",
    "\n",
    "# Initialize variables to store the best parameters\n",
    "best_gamma = None\n",
    "best_threshold = None\n",
    "best_f1 = 0\n",
    "best_w = None\n",
    "\n",
    "# Loop over gamma values\n",
    "for gamma in gamma_values:\n",
    "    print(f\"Testing gamma: {gamma}\")\n",
    "    # Initialize weights\n",
    "    initial_w = np.zeros(X_train.shape[1])\n",
    "    # Train the model\n",
    "    w, loss = mean_squared_error_sgd(y_train, X_train, initial_w, max_iters=1000, gamma=gamma)\n",
    "\n",
    "    # Compute predicted scores on the validation set\n",
    "    y_scores = X_test @ w\n",
    "\n",
    "    # Optimize threshold for the current gamma\n",
    "    threshold, f1, acc = optimize_threshold(y_test, y_scores)\n",
    "\n",
    "    print(f\"Gamma: {gamma}, Best Threshold: {threshold}, F1 Score: {f1}, Accuracy: {acc}\")\n",
    "\n",
    "    # Update best parameters if current F1 is better\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_gamma = gamma\n",
    "        best_threshold = threshold\n",
    "        best_w = w.copy()\n",
    "\n",
    "# After finding the best gamma and threshold, make predictions on the test set\n",
    "y_scores = X_test @ best_w\n",
    "y_prediction = np.where(y_scores >= best_threshold, 1, -1)\n",
    "\n",
    "# Compute final accuracy and F1 score\n",
    "final_f1_score = f1_score(y_test, y_prediction)\n",
    "\n",
    "print(\"Best Gamma:\", best_gamma)\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Final F1 Score:\", final_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehdibouchoucha/Desktop/ML/AmelProject/implementations.py:105: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w=np.linalg.lstsq(A,b)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: -0.5670693947456843\n",
      "Final Accuracy: 0.8651926798421381\n",
      "Final F1 Score: 0.41313432835820896\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(y_train, X_train)\n",
    "\n",
    "y_scores = X_test @ w\n",
    "\n",
    "# Optimize threshold for the current gamma\n",
    "threshold, f1, acc = optimize_threshold(y_test, y_scores)\n",
    "\n",
    "# After finding the best gamma and threshold, make predictions on the test set\n",
    "y_scores = X_test @ w\n",
    "y_prediction = np.where(y_scores >= threshold, 1, -1)\n",
    "\n",
    "# Compute final accuracy and F1 score\n",
    "final_accuracy = accuracy(y_test, y_prediction)\n",
    "final_f1_score = f1_score(y_test, y_prediction)\n",
    "\n",
    "print(\"Best Threshold:\", threshold)\n",
    "print(\"Final Accuracy:\", final_accuracy)\n",
    "print(\"Final F1 Score:\", final_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing lambda: 0.005\n",
      "Gamma: 0.01, Best Threshold: -0.5786245959336018, F1 Score: 0.4106614017769003, Accuracy: 0.8635470157099974\n",
      "Testing lambda: 0.01\n",
      "Gamma: 0.01, Best Threshold: -0.5782330978537572, F1 Score: 0.41038171971998416, Accuracy: 0.8639584317430326\n",
      "Testing lambda: 0.015\n",
      "Gamma: 0.01, Best Threshold: -0.5778854457415188, F1 Score: 0.41129673826571206, Accuracy: 0.864689838023984\n",
      "Testing lambda: 0.02\n",
      "Gamma: 0.01, Best Threshold: -0.5753453598351297, F1 Score: 0.41125251172136634, Accuracy: 0.866061224800768\n",
      "Best Lambda: 0.015\n",
      "Best Threshold: -0.5778854457415188\n",
      "Final F1 Score: 0.41129673826571206\n",
      "Final Accuracy: 0.864689838023984\n"
     ]
    }
   ],
   "source": [
    "# Define a range of lambda values to test\n",
    "lambda_values = [0.005, 0.01 , 0.015, 0.02]\n",
    "\n",
    "# Initialize variables to store the best parameters\n",
    "best_lambda = None\n",
    "best_threshold = None\n",
    "best_f1 = -1\n",
    "best_w = None\n",
    "\n",
    "# Loop over lambda values\n",
    "for lambda_ in lambda_values:\n",
    "    print(f\"Testing lambda: {lambda_}\")\n",
    "    # Train the model\n",
    "    w, loss = ridge_regression(y_train, X_train, lambda_)\n",
    "    \n",
    "    # Compute predicted scores on the test set\n",
    "    y_scores = X_test @ w  # Continuous scores\n",
    "    \n",
    "    # Optimize threshold for the current lambda\n",
    "    threshold, f1, acc = optimize_threshold(y_test, y_scores)\n",
    "    \n",
    "    print(f\"Gamma: {gamma}, Best Threshold: {threshold}, F1 Score: {f1}, Accuracy: {acc}\")\n",
    "    \n",
    "    # Update best parameters if current F1 is better\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_lambda = lambda_\n",
    "        best_threshold = threshold\n",
    "        best_w = w.copy()\n",
    "\n",
    "# After finding the best lambda and threshold, make predictions on the test set\n",
    "y_scores = X_test @ best_w\n",
    "y_pred = np.where(y_scores >= best_threshold, 1, -1)\n",
    "\n",
    "# Compute final F1 score\n",
    "final_f1_score = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Lambda:\", best_lambda)\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Final F1 Score:\", final_f1_score)\n",
    "print(\"Final Accuracy:\", accuracy(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gamma: 0.9\n",
      "Current iteration=0, loss=1.580226449126937\n",
      "Current iteration=100, loss=0.32482471721749684\n",
      "Current iteration=200, loss=0.3391803220447881\n",
      "Current iteration=300, loss=0.2693260509636259\n",
      "Current iteration=400, loss=0.2942305602379432\n",
      "Current iteration=500, loss=0.2557766921885954\n",
      "Current iteration=600, loss=0.28532196319066044\n",
      "Current iteration=700, loss=0.2554614885016538\n",
      "Current iteration=800, loss=0.2843560949045877\n",
      "Current iteration=900, loss=0.25585764983762965\n",
      "Gamma: 0.9, Best Threshold: 0.30303030303030304, F1 Score: 0.42230278784848113, Accuracy: 0.859173815655142\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m y_prediction \u001b[38;5;241m=\u001b[39m y_prediction_binary \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Convert back to -1 and 1\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Compute final accuracy and F1 score\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m final_accuracy \u001b[38;5;241m=\u001b[39m accuracy(y_test, y_prediction)\n\u001b[1;32m     44\u001b[0m final_f1_score \u001b[38;5;241m=\u001b[39m f1_score_logistic(y_test_binary, y_prediction_binary)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Gamma:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_gamma)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "# Convert y_train and y_test to binary labels (0 and 1)\n",
    "y_train_binary = (y_train + 1) // 2\n",
    "y_test_binary = (y_test + 1) // 2\n",
    "\n",
    "# Define a range of gamma values to test\n",
    "gamma_values = [0.9]\n",
    "\n",
    "# Initialize variables to store the best parameters\n",
    "best_gamma = None\n",
    "best_threshold = None\n",
    "best_f1 = 0\n",
    "best_w = None\n",
    "\n",
    "# Loop over gamma values\n",
    "for gamma in gamma_values:\n",
    "    print(f\"Testing gamma: {gamma}\")\n",
    "    # Initialize weights\n",
    "    initial_w = np.zeros(X_train.shape[1])\n",
    "    # Train the model\n",
    "    w, loss = logistic_regression(y_train_binary, X_train, initial_w, max_iters=1000, gamma=gamma)\n",
    "    \n",
    "    # Compute predicted probabilities on the validation set\n",
    "    y_scores = sigmoid(X_test @ w)\n",
    "    \n",
    "    # Optimize threshold for the current gamma\n",
    "    threshold, f1, acc = optimize_threshold_logistic(y_test_binary, y_scores)\n",
    "    \n",
    "    print(f\"Gamma: {gamma}, Best Threshold: {threshold}, F1 Score: {f1}, Accuracy: {acc}\")\n",
    "    \n",
    "    # Update best parameters if current F1 is better\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_gamma = gamma\n",
    "        best_threshold = threshold\n",
    "        best_w = w.copy()\n",
    "\n",
    "# After finding the best gamma and threshold, make predictions on the test set\n",
    "y_scores = sigmoid(X_test @ best_w)\n",
    "y_prediction_binary = np.where(y_scores >= best_threshold, 1, 0)\n",
    "y_prediction = y_prediction_binary * 2 - 1  # Convert back to -1 and 1\n",
    "\n",
    "# Compute final accuracy and F1 score\n",
    "final_f1_score = f1_score_logistic(y_test_binary, y_prediction_binary)\n",
    "\n",
    "print(\"Best Gamma:\", best_gamma)\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Final F1 Score:\", final_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gamma: 0.9, lambda: 0.0001\n",
      "Current iteration=0, loss=1.580926190366109 (with regularization)\n",
      "Current iteration=100, loss=0.25310248661438456 (with regularization)\n",
      "Current iteration=200, loss=0.3810539437599975 (with regularization)\n",
      "Current iteration=300, loss=0.36750626878536896 (with regularization)\n",
      "Current iteration=400, loss=0.38258974982361066 (with regularization)\n",
      "Current iteration=500, loss=0.40461582835656396 (with regularization)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m initial_w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m w, loss \u001b[38;5;241m=\u001b[39m reg_logistic_regression(\n\u001b[1;32m     24\u001b[0m     y_train_binary, X_train, lambda_, initial_w, max_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, gamma\u001b[38;5;241m=\u001b[39mgamma\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Compute predicted probabilities on the test set\u001b[39;00m\n\u001b[1;32m     28\u001b[0m y_scores \u001b[38;5;241m=\u001b[39m sigmoid(X_test \u001b[38;5;241m@\u001b[39m w)\n",
      "File \u001b[0;32m~/Desktop/ML/AmelProject/implementations.py\u001b[0m, in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Convert y_train and y_test to binary labels (0 and 1)\n",
    "y_train_binary = (y_train + 1) // 2\n",
    "y_test_binary = (y_test + 1) // 2\n",
    "\n",
    "# Define a range of gamma and lambda values to test\n",
    "gamma_values = [0.9]\n",
    "lambda_values = [0.0001]\n",
    "\n",
    "# Initialize variables to store the best parameters\n",
    "best_gamma = None\n",
    "best_lambda = None\n",
    "best_threshold = None\n",
    "best_f1 = -1\n",
    "best_w = None\n",
    "\n",
    "# Loop over gamma and lambda values\n",
    "for gamma in gamma_values:\n",
    "    for lambda_ in lambda_values:\n",
    "        print(f\"Testing gamma: {gamma}, lambda: {lambda_}\")\n",
    "        # Initialize weights\n",
    "        initial_w = np.zeros(X_train.shape[1])\n",
    "        # Train the model\n",
    "        w, loss = reg_logistic_regression(\n",
    "            y_train_binary, X_train, lambda_, initial_w, max_iters=10, gamma=gamma\n",
    "        )\n",
    "        \n",
    "        # Compute predicted probabilities on the test set\n",
    "        y_scores = sigmoid(X_test @ w)\n",
    "        \n",
    "        # Optimize threshold for the current gamma and lambda\n",
    "        threshold, f1, acc = optimize_threshold_logistic(y_test_binary, y_scores)\n",
    "        \n",
    "        print(f\"Gamma: {gamma}, Lambda: {lambda_}, Best Threshold: {threshold}, F1 Score: {f1}, Accuracy: {acc}\")\n",
    "        \n",
    "        # Update best parameters if current F1 is better\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_gamma = gamma\n",
    "            best_lambda = lambda_\n",
    "            best_threshold = threshold\n",
    "            best_w = w.copy()\n",
    "\n",
    "# After finding the best gamma, lambda, and threshold, make predictions on the test set\n",
    "y_scores = sigmoid(X_test @ best_w)\n",
    "y_prediction_binary = np.where(y_scores >= best_threshold, 1, 0)\n",
    "y_prediction = y_prediction_binary * 2 - 1  # Convert back to -1 and 1\n",
    "\n",
    "# Compute final accuracy and F1 score\n",
    "final_f1_score = f1_score_logistic(y_test_binary, y_prediction_binary)\n",
    "\n",
    "print(\"Best Gamma:\", best_gamma)\n",
    "print(\"Best Lambda:\", best_lambda)\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Final F1 Score:\", final_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=1.580926190366109 (with regularization)\n",
      "Current iteration=100, loss=0.25310248661438456 (with regularization)\n",
      "Current iteration=200, loss=0.3810539437599975 (with regularization)\n",
      "Current iteration=300, loss=0.36750626878536896 (with regularization)\n",
      "Current iteration=400, loss=0.38258974982361066 (with regularization)\n",
      "Current iteration=500, loss=0.40461582835656396 (with regularization)\n",
      "Current iteration=600, loss=0.3800112928687103 (with regularization)\n",
      "Current iteration=700, loss=0.40224673244121306 (with regularization)\n",
      "Current iteration=800, loss=0.3720865397839348 (with regularization)\n",
      "Current iteration=900, loss=0.39900342153832924 (with regularization)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert y labels from {-1, 1} to binary {0, 1} for training and validation sets\n",
    "y_train_binary = (y_train + 1) // 2\n",
    "y_test_binary = (y_test + 1) // 2\n",
    "\n",
    "# Initialize parameters for the model\n",
    "best_lambda = 0.0001\n",
    "best_gamma = 0.9                        # Learning rate for the logistic regression model\n",
    "initial_w = np.zeros(X_train.shape[1])  # Initial weights set to zeros\n",
    "\n",
    "# Train the logistic regression model\n",
    "w, loss = reg_logistic_regression(y_train_binary, X_train, best_lambda, initial_w, max_iters=1000, gamma=best_gamma)\n",
    "\n",
    "# Compute prediction scores for the test set\n",
    "y_scores = sigmoid(x_test @ w)  # Apply the sigmoid function to compute probabilities\n",
    "\n",
    "# Optimize the threshold to maximize F1 score and accuracy\n",
    "#best_threshold, best_f1, best_accuracy = optimize_threshold_logistic(y_test_binary, y_scores)\n",
    "\n",
    "# Generate binary predictions based on the best threshold\n",
    "y_pred = np.where(y_scores >= 0.393939393939394, 1, -1)\n",
    "\n",
    "# Output the performance results\n",
    "#print(\"Threshold:\", best_threshold)\n",
    "#print(\"Final Accuracy:\", best_accuracy)\n",
    "#print(\"Final F1 Score:\", best_f1)\n",
    "\n",
    "# Create a CSV submission file with the predictions\n",
    "create_csv_submission(test_ids, y_pred, \"new_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "import numpy as np\n",
    "from implementations import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(data_path, sub_sample=False):\n",
    "    \"\"\"\n",
    "    This function loads the data and returns the respectinve numpy arrays.\n",
    "    Remember to put the 3 files in the same folder and to not change the names of the files.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): datafolder path\n",
    "        sub_sample (bool, optional): If True the data will be subsempled. Default to False.\n",
    "\n",
    "    Returns:\n",
    "        x_train (np.array): training data\n",
    "        x_test (np.array): test data\n",
    "        y_train (np.array): labels for training data in format (-1,1)\n",
    "        train_ids (np.array): ids of training data\n",
    "        test_ids (np.array): ids of test data\n",
    "    \"\"\"\n",
    "    y_train = np.genfromtxt(\n",
    "        os.path.join(data_path, \"y_train.csv\"),\n",
    "        delimiter=\",\",\n",
    "        skip_header=1,\n",
    "        dtype=int,\n",
    "        usecols=1,\n",
    "    )\n",
    "    x_train = np.genfromtxt(\n",
    "        os.path.join(data_path, \"x_train_cleaned.csv\"), delimiter=\",\", skip_header=1\n",
    "    )\n",
    "    x_test = np.genfromtxt(\n",
    "        os.path.join(data_path, \"x_test_cleaned.csv\"), delimiter=\",\", skip_header=1\n",
    "    )\n",
    "\n",
    "    train_ids = x_train[:, 0].astype(dtype=int)\n",
    "    test_ids = x_test[:, 0].astype(dtype=int)\n",
    "    x_train = x_train[:, 1:]\n",
    "    x_test = x_test[:, 1:]\n",
    "\n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        y_train = y_train[::50]\n",
    "        x_train = x_train[::50]\n",
    "        train_ids = train_ids[::50]\n",
    "\n",
    "    return x_train, x_test, y_train, train_ids, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"dataset\", sub_sample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w=np.zeros((x_train.shape[1],))\n",
    "gamma=0.1\n",
    "max_iters=100\n",
    "y_train = y_train[train_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy between true labels and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (numpy array): True labels\n",
    "    y_pred (numpy array): Predicted labels\n",
    "\n",
    "    Returns:\n",
    "    float: Accuracy score\n",
    "    \"\"\"\n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    total = len(y_true)\n",
    "    return correct / total\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the F1-score between true labels and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (numpy array): True labels\n",
    "    y_pred (numpy array): Predicted labels\n",
    "\n",
    "    Returns:\n",
    "    float: F1-score\n",
    "    \"\"\"\n",
    "    # True Positives (TP)\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    # False Positives (FP)\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    # False Negatives (FN)\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    # Avoid division by zero\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "    if (precision + recall) == 0:\n",
    "        return 0\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_mse2(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    N = len(y)\n",
    "    error = y - tx.dot(w)\n",
    "    gradient = -1 / N * tx.T.dot(error)\n",
    "    return gradient\n",
    "\n",
    "def compute_loss_mse2(y, tx, w):\n",
    "    \"\"\"Calculate the loss using mse.\"\"\"\n",
    "    N = len(y)\n",
    "    squared_error = (y - tx.dot(w)) ** 2\n",
    "    loss = 1 / (2 * N) * np.sum(squared_error)\n",
    "    return loss\n",
    "\n",
    "def mean_squared_error_gd2(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Calculate the loss using mse\"\"\"\n",
    "    if max_iters == 0:\n",
    "        return (initial_w, compute_loss_mse2(y, tx, initial_w))\n",
    "    w = initial_w\n",
    "    # loss = compute_loss_mse(y, tx, w)\n",
    "    for iter in range(max_iters):\n",
    "        gradient = compute_gradient_mse2(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "        if iter % 100 == 0:\n",
    "            print(\n",
    "                \"Current iteration={i}, loss={l}\".format(\n",
    "                    i=iter, l=compute_loss_mse2(y, tx, w)\n",
    "                )\n",
    "            )\n",
    "    loss = compute_loss_mse2(y, tx, w)\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.4725981665543405\n",
      "Current iteration=100, loss=0.15143367794530063\n",
      "Current iteration=200, loss=0.14740982820692666\n",
      "Current iteration=300, loss=0.14504751612216937\n",
      "Current iteration=400, loss=0.1435365633036567\n"
     ]
    }
   ],
   "source": [
    "w, loss = mean_squared_error_gd2(y_train, X_train, initial_w, 500, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9129321773050726"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prediction = X_test @ w\n",
    "y_prediction = np.where(y_prediction > 0.6, 1, -1) \n",
    "score = accuracy(y_test, y_prediction)\n",
    "score\n",
    "#print(y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = f1_score(y_test, y_prediction)\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
